{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Networks and Deep Learning Theory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- AI is the new Electricity\n",
    "¿Qué aprenderemos?:\n",
    "\n",
    "Principios acerca de Deep Learning, como crear redes neuronales. \n",
    "Haremos un algoritmo para crear reconocimientos de gatos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Week 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction to neural networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Podemos utilizar una red neuronal para predecir los precios de las casas a partir del tamaño de una casa.\n",
    "Normalmente, utilizaríamos un modelo de regresión para hacer fit sobre una línea.\n",
    "\n",
    "- Sin embargo, si usamos una neurona, tendrá como entrada x el tamaño de la casa y como salida la Y, que sería el precio. Sabiendo que estos precios jamás serán negativos.\n",
    "\n",
    "- Está función que inicia desde cero, es utilizada bastante en redes neuronales:\n",
    "\n",
    "- **ReLU Functions**: Significa una Rectified Linear Unit- Empieza desde cero y empieza a hacer la predicción."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src = 'House_Predictions.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En estos casos, en vez del ejemplo al lado el título, la red neuronal define que tipo de características es esta. Adicional, cada una de las características que conectan al precio, pasa por todas las redes neuronales"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Supervised Learning With Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Significa, en este caso, que uno escoge que será el input y el output de la red neuronal. Y obteniendo así los resultados esperados.\n",
    "\n",
    "Para algunas aplicaciones, es conveniente utilizar una o diferentes tipos de redes neuronales:\n",
    "\n",
    "- **Standard NN**: Predecir series de tiempo, precios, ads, información de usuarios.\n",
    "\n",
    "- **Convolutional Neural Networks**: Detección y segmentación de imágenes\n",
    "\n",
    "- **Recurrent Neural Network**: Audios, Traducción"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src = 'Neural_Networks_Types.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En los últimos tiempos, se ha pasado en las redes neuronales, de funciones sigmoides a funciones RELU que son capaces de tener mejor performance en el entrenamiento de redes neuronales.\n",
    "\n",
    "Hace que el descenso del gradiente sea mucho mejor y más rápido utilizando RELU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Week 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En esta semana, se aprenderá a configurar un problema de machine learning con redes neuronales y utilizando vectorización para hacer modelos más rápidos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clasificación binaria"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aprenderé de dos conceptos importantes: **Forward Propagation** y **Backward Propagation**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src = 'Binary_1.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cuando hacemos clasificación binaria, por ejemplo, de una imagen, si tenemos una imagen de 64x64 pixeles, esta puede ser dividida en 3 matrices RGB, cada matriz tiene las combinaciones para generar el color adecuado.\n",
    "\n",
    "En este caso, nuestra clasificación binaria **Y** nos dará como resultado: 1 es un gato, 0 no es un gato.\n",
    "\n",
    "Para ello, tenemos un vector **X** que tomará como entrada, todas combinaciones de RGB, el tamaño de este vector se denota como **n** o **nx**. Al tener 3 matrices, cada una de 64x64, en total tendríamos un vector de tamaño 12.288\n",
    "\n",
    "La idea de este modelo, es que al pasar el vector **X** con la información RGB de la imagen, nos devuelva un **Y** el cual es un binario y nos dirá 1 si es un gatito, o 0 si no lo es"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Notación utilizada:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src = 'Binary_2.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La notación utilizada en el curso, corresponde a lo siguiente:\n",
    "\n",
    "- (x, y), donde x pertenece a los reales de tamaño n, e y que está en el conjunto de 0 a 1\n",
    "- Tenemos **m** training examples, los cuales son {(x1, y1), (x2, y2) ... (xn, yn)}\n",
    "- Donde **m** es todo el training, y **mtest**, es el número de test examples\n",
    "\n",
    "Por otro aldo, tenemos las matrices y vectores:\n",
    "- Dónde X es una matriz de m x n\n",
    "- Dónde Y, es un vector horizontal (1, m) que pertenece a los reales de dimensión 1 x m\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logistic Regression Definition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Este modelo, se utiliza cuando se hace clasificación binaria.\n",
    "\n",
    "Dado un x, queremos un  $\\hat{y} = P(y = 1 | x)$ donde $0 \\leq \\hat{y} \\leq 1$, $\\hat{y}$ nos dice la probabilidad de que sea o no un gato.\n",
    "\n",
    "Dónde $\\ x \\in R^n$\n",
    "\n",
    "- Parámetros: $\\ w \\in R^n, b \\in R$. <br>\n",
    "¿Cómo generamos una función que nos devuelva $\\hat{y}$?, algo que podríamos intentar pero non funcionaría sería lo siguiente:\n",
    "\n",
    "- Salida: $\\hat{y} = w^Tx + b$, esta función toma la traspuesta w, el cual sería bueno si estariamos haciendo una regresión lineal, pero, esto incluso nos daría valores negativo, pero, no tendría sentido para lo que quermos\n",
    "\n",
    "Ahora bien, para tener una salida, podemos aplicar una función sigmoide a la ecuación anterior, esta función empieza desde cero y corta en 0.5, quedando de la siguiente forma:\n",
    "\n",
    "- Salida: $\\hat{y} = \\sigma(w^Tx + b)$, denotaremos a lo que se encuentra dentro de la función, como z.\n",
    "\n",
    "La función sigmoide estaría definida de la siguiente forma: $\\sigma(z) = \\frac{1}{1+e^{-z}}$.\n",
    "\n",
    "- Si z es un valor muy grande, da aproximadamente 1.\n",
    "- Si z es un valor muy pequeño, o negativo, da aproximadamente 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logistic Regression Cost Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para poder determinar los parámetros de w y b, debemos generar una función de costo, lo cual será lo siguiente dentro de la explicación:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Loss Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En general, lo que hicimos la vez pasada, es lo siquiente:\n",
    "\n",
    "- $\\hat{y} = \\sigma(w^{T}x + b)$ dónde $\\sigma(z) = \\frac{1}{1+e^{-z}}$\n",
    "\n",
    "- Dado un $\\lbrace (x^{(1)}, y^{(1)}),..., (x^{(m)}, y^{(m)}) \\rbrace$, queremos, $\\hat{y}^{(i)} \\approx y^{(i)}$\n",
    "\n",
    "- Dónde i es el i-th training example\n",
    "\n",
    "Ahora bien, la función de pérdida, o Loss error function, sería:\n",
    "- $L(\\hat{y},y) = \\frac{1}{2}(\\hat{y}-y)^2$\n",
    "\n",
    "Esta función, no es más que el MSE: Mean Squared Error. Pero, en regresión logística, esto no se utiliza, ya que al minimizarlo, no converge a un óptimo. Es decir, en este caso, estaríamos con múltiples óptimos locales y queremos uno global. Entonces, usando descenso del gradiente, no convergerá a un óptimo global.\n",
    "\n",
    "En ese caso, usaremos una función de pérdida diferente, que pueda converger a un óptimo global:\n",
    "- $L(\\hat{y},y) = -(ylog(\\hat{y})+(1-y)log(1-\\hat{y}))$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Cost Function\n",
    "\n",
    "Si nos damos cuenta, esto sirve para evaluar 1 a 1 cada ejemplo del training set, para medir nuestro verdadero performance del modelo, debemos definir una función de costo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Función de costo:\n",
    "\n",
    "- $J(w, b) = \\frac{1}{m} \\sum_{i=1}^{m}L(\\hat{y}^{(i)},y^{(i)})$\n",
    "\n",
    "Expandiendo la función, sería igual a lo siguiente:\n",
    "- $J(w, b) = -\\frac{1}{m} \\sum_{i=1}^{m}[y^{(i)}log(\\hat{y}^{(i)})+(1-y^{(i)})log(1-\\hat{y}^{(i)})]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Descenso del Gradiente (Teoría)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recapitulando, tenemos las siguientes funciones:\n",
    "\n",
    "- $\\hat{y} = \\sigma(w^{T}x + b)$ dónde $\\sigma(z) = \\frac{1}{1+e^{-z}}$\n",
    "\n",
    "<br>\n",
    "\n",
    "$J(w, b) = \\frac{1}{m} \\sum_{i=1}^{m}L(\\hat{y}^{(i)},y^{(i)}) = -\\frac{1}{m} \\sum_{i=1}^{m}[y^{(i)}log(\\hat{y}^{(i)})+(1-y^{(i)})log(1-\\hat{y}^{(i)})]$\n",
    "\n",
    "Queremos encontrar w, b que minimiza J(w, b)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src = 'Descent_1.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tomando a w como eje de ejemplo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usando como ejemplo el eje de w, lo que haremos con el algoritmo del descenso del gradiente, será lo siguiente.\n",
    "\n",
    "<img src = 'Descent_2.png'>\n",
    "\n",
    "Repetidamente: $w:=w-\\alpha \\frac{dJ(w)}{dw}$, dónde (:=) significa actualizar w, recordemos que la derivada de la función, nos dá la pendiente de la recta tangente al punto w. \n",
    "\n",
    "Dónde $\\alpha$ es el learning rate. Nuestra derivada de J respecto a w, será representada en el código como dw, siendo esta igual a $\\frac{dJ(w)}{dw}$, dejando la ecuación como $w:=w-\\alpha dw$\n",
    "\n",
    "Adicional, recordemos que si evaluamos la en un punto donde la curva crece, la pendiente será positiva, si es en un punto donde decrece, la pendiente será negativa. Queremos que, la pendiente sea completamente paralela al eje de la w, por lo cual, buscaremos que esta derivada, de 0. Así, estaríamos convergiendo al óptimo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Actualizando w y b en nuestro algoritmo de descenso del gradiente"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora bien, el ejemplo anterior fue con w solamente, en este caso, si queremos incluir ambos parámetros, hacemos lo siguiente:\n",
    "\n",
    "- $w:=w-\\alpha \\frac{dJ(w, b)}{dw}$\n",
    "\n",
    "<br>\n",
    "\n",
    "- $b:=b-\\alpha \\frac{dJ(w, b)}{db}$\n",
    "\n",
    "Ahora bien, en el código, db también sería la derivada de acuerdo con nuestra notación, quedando la ecuación de la siguiente forma:\n",
    "\n",
    "\n",
    "- $w:=w-\\alpha dw$\n",
    "\n",
    "<br>\n",
    "\n",
    "- $b:=b-\\alpha db$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computation Graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dentro de nuestras redes neuronales, tenemos dos pasos:\n",
    "\n",
    "- **Forward propagation step**: En este, cálculamos la salida o respuesta de nuestra red neuronal.\n",
    "\n",
    "- **Backward propagation step**: En este, calculamos los gradientes y las derivadas y todas nuestras métricas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En esta clase, dvar representa la derivada de una respuesta final de una variable, respecto a varias cantidades intermedias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Descenso del Gradiente aplicado a regresión logística:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Descenso del gradiente en un sólo example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- $z = w^Tx + b$\n",
    "- $\\hat{y} = a = \\sigma(z)$\n",
    "- $L(a, y) = -(ylog(a)+(1-y)log(1-a))$\n",
    "\n",
    "En una gráfica de computación, se vé de la siguiente manera:\n",
    "\n",
    "<img src = 'Descent_3.png'>\n",
    "\n",
    "En este caso, queremos, los parámetros de w, y b que nos puedan ayudar a minimizar la función de pérdida.\n",
    "\n",
    "En computation graph, vamos de adelante para atrás:\n",
    "\n",
    "<img src = 'Descent_4.png'>\n",
    "\n",
    "Se calculan todos los dz, db, dw y se pueden actualizar todos los pesos de la siguiente forma:\n",
    "\n",
    "- $w_1:=w_1-\\alpha dw_1$\n",
    "\n",
    "<br>\n",
    "\n",
    "- $w_2:=w_2-\\alpha dw_2$\n",
    "\n",
    "<br>\n",
    "\n",
    "- $b:=b-\\alpha db$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Descenso del gradiente en m examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Debemos recordar como está compuesta nuestra fórmula de costo, en este caso, reemplazando $\\hat{y}$ por a como definimos anteriormente:\n",
    "\n",
    "- $J(w, b) = \\frac{1}{m} \\sum_{i=1}^{m}L(a^{i},y^{(i)})$\n",
    "\n",
    "Ahora bien, en código, podemos hacer lo siguiente:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "J, dw1, dw2, db = [0, 0, 0, 0]\n",
    "def sigma(z):\n",
    "    return None\n",
    "    \n",
    "x1 = []\n",
    "x2 = []\n",
    "m = []\n",
    "z = []\n",
    "a = []\n",
    "dz = []\n",
    "y = []\n",
    "wt = 'matriz traspuesta'\n",
    "b= 'algo'\n",
    "for i in m:\n",
    "    z[i] = wt*x1[i] + b\n",
    "    a[i] = sigma(z[i])\n",
    "    J += -(y[i]*np.log(a[i])+(1-y[i])*np.log(1-a[i]))\n",
    "    dz[i] = a[i]-y[i]\n",
    "    dw1 += x1[i]*dz[i]\n",
    "    dw2 += x2[i]*dz[i]\n",
    "    db += dz[i]\n",
    "    J /= m\n",
    "    dw1 /= m\n",
    "    dw2 /= m\n",
    "    db /= m\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectorization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Esto nos permite incluir código cuyos tiempos de carga sean más rápidos. Esto gracias a las librerías de numpy en Python, hagamos un demo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 2, 3, 4])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "a = np.array([i for i in range(1, 5)])\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vectorized version: 1.0097026824951172 ms\n",
      "249888.29609155605\n",
      "249888.29609154782\n",
      "For loop: 231.04047775268555 ms\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "a = np.random.rand(1000000)\n",
    "b = np.random.rand(1000000)\n",
    "\n",
    "tic = time.time()\n",
    "c = np.dot(a, b)\n",
    "toc = time.time()\n",
    "\n",
    "print(\"Vectorized version: \"+str(1000*(toc-tic)) + ' ms')\n",
    "print(c)\n",
    "\n",
    "c = 0\n",
    "tic = time.time()\n",
    "for i in range(1000000):\n",
    "    c += a[i]*b[i]\n",
    "\n",
    "toc = time.time()\n",
    "print(c)\n",
    "print(\"For loop: \"+str(1000*(toc-tic)) + ' ms')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 56.    0.    4.4  68. ]\n",
      " [  1.2 104.   52.    8. ]\n",
      " [  1.8 135.   99.    0.9]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "A = np.array([[56.0, 0.0, 4.4, 68.0],\n",
    "              [1.2, 104.0, 52.0, 8.0],\n",
    "              [1.8, 135.0, 99.0, 0.9]])\n",
    "\n",
    "print(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 59.  239.  155.4  76.9]\n"
     ]
    }
   ],
   "source": [
    "cal = A.sum(axis=0)\n",
    "print(cal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[94.91525424,  0.        ,  2.83140283, 88.42652796],\n",
       "       [ 2.03389831, 43.51464435, 33.46203346, 10.40312094],\n",
       "       [ 3.05084746, 56.48535565, 63.70656371,  1.17035111]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "percentage = 100*A/cal.reshape(1,4)\n",
    "percentage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Coding in Numpy with Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Primero que todo, recordemos que una función sigmoide se define de la siguiente manera:\n",
    "\n",
    "- $\\sigma(x) = \\frac{1}{1+e^{-x}}$\n",
    "\n",
    "Debemos escribir una función que tome un número cualquiera y nos devuelva su cálculo de la función sigmoide:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9820137900379085"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import math\n",
    "def sigmoide(x: float) -> float:\n",
    "    return 1/(1+math.exp(-x))\n",
    "\n",
    "sigmoide(4.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora bien, en temas de redes neuronales, estaremos trabajando con vectores, si queremos generar una función sigmoide para un vector, debe aprovecharse las bondades de numpy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.73105858, 0.88079708, 0.95257413])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "def sigmoide_vectors(x : np.array)->np.array:\n",
    "    \"\"\"\n",
    "    Devuelve los valores de la función sigmoide de un vector:\n",
    "\n",
    "    Argumentos:\n",
    "    x(Array): Es un vector de tamaño n\n",
    "\n",
    "    Return:\n",
    "    Array: Función sigmoide cálculada para un vector de tamaño n\n",
    "    \"\"\"\n",
    "    return 1/(1+np.exp(-x))\n",
    "\n",
    "t_x = np.array([1, 2, 3])\n",
    "\n",
    "sigmoide_vectors(t_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora bien, podemos graficar con matplotlib el vector de entrada y el vector de arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Así se vería una función sigmoide')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "## Definimos un input array como vector de entrada de nuestra función sigmoide:\n",
    "input_array = np.array([i for i in range(-15, 16)])\n",
    "output_array = sigmoide_vectors(input_array)\n",
    "plt.plot(output_array)\n",
    "plt.title('Así se vería una función sigmoide')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Week 2: Assignment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En esta parte, se escribirán todas las funciones y paquetes utilizados para el algoritmo de descenso del gradiente. Es importante que, de ser posible, se cuente con una versión gráfica. Esta se puede ver más que todo en Platzi. Recordar el curso de Matemáticas para DS. Traer ese ejercicio a este espacio más adelante y repasarlo.\n",
    "\n",
    "La idea general, de lo que recuerdo, es la siguiente, debemos, de la función de costo/pérdida, minimizarla hasta que converga a un óptimo global.\n",
    "\n",
    "En este caso, se está construyendo un modelo de regresión logística que sea capaz de identificar si una imagen es un gato o no es un gato."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nuestro DataSet de imágenes:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Es importante resaltar, que nuestro dataset de imágenes tiene colores. Las máquinas, leen las imágenes como una combinación de 3 matrices: RGB, cada matriz es de un tamaño de pixeles específicos y contiene un número entre 0 y 255. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Entonces, debemos generar un vector, cuyas dimensiones sean: $(m*num_px*num_px*3, 1)$ dónde m es el número de fotos con las que contamos, num_px es el número de pixeles y 3 son las tres matrices RGB.\n",
    "\n",
    "Para ello, un truco es hacer lo siguiente:\n",
    "\n",
    "```python\n",
    "\n",
    "Vector = Vector.reshape(Vector.shape[0], -1).T\n",
    "\n",
    "```\n",
    "\n",
    "Adicionalmente, es importante que nuestros vectores, tanto de test como de entrenamiento, estén normalizados. Para ello, se dividen entre 255."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ecuaciones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora bien, recordemos que estaremos usando las siguientes ecuaciones:\n",
    "\n",
    "$$z^{(i)} = w^T x^{(i)} + b \\tag{1}$$\n",
    "$$\\hat{y}^{(i)} = a^{(i)} = sigmoid(z^{(i)})\\tag{2}$$ \n",
    "$$ \\mathcal{L}(a^{(i)}, y^{(i)}) =  - y^{(i)}  \\log(a^{(i)}) - (1-y^{(i)} )  \\log(1-a^{(i)})\\tag{3}$$\n",
    "\n",
    "Lo que nos dará, como resultado la siguiente ecuación de costo:\n",
    "\n",
    "$$ J = \\frac{1}{m} \\sum_{i=1}^m \\mathcal{L}(a^{(i)}, y^{(i)})\\tag{6}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En la siguiente parte, vamos a crear una a una cada función que nos ayudará en la creación de nuestro algoritmo:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ecuación sigmoide:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    return 1/(1+np.exp(-z))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inicializando con ceros las variables de w y b:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_with_zeros(dim):\n",
    "    \"\"\"Recordemos:\n",
    "    dim: no es más que el tamaño de nuestro w, en anteriores apuntes, definimos que w es de tamaño\n",
    "    nx\n",
    "    \"\"\"\n",
    "    w = np.zeros((dim, 1))\n",
    "    b = 0.0\n",
    "    return w, b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Propagación hacia delante y hacia atrás:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En esta parte, hacemos todo el cálculo desde inicio a fin, sin calcular las métricas de nuestro modelo. Recordemos las ecuaciones en este caso.\n",
    "\n",
    "Hacia delante, recordemos que es todo el cálculo normal:\n",
    "\n",
    "- Tenemos X\n",
    "- Calculamos $A = \\sigma(w^T X + b) = (a^{(1)}, a^{(2)}, ..., a^{(m-1)}, a^{(m)})$\n",
    "- Luego, la función de costo: $J = -\\frac{1}{m}\\sum_{i=1}^{m}(y^{(i)}\\log(a^{(i)})+(1-y^{(i)})\\log(1-a^{(i)}))$\n",
    "\n",
    "Hacia atrás, calculamos las derivadas parciales de cada función que ya tenemos definidas:\n",
    "\n",
    "$$ \\frac{\\partial J}{\\partial w} = \\frac{1}{m}X(A-Y)^T\\tag{7}$$\n",
    "$$ \\frac{\\partial J}{\\partial b} = \\frac{1}{m} \\sum_{i=1}^m (a^{(i)}-y^{(i)})\\tag{8}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def propagate(w, b, X, Y):\n",
    "    #--Propagación hacia delante:\n",
    "    m = X.shape[1]\n",
    "    A = sigmoid(np.dot(w.T, X)+b)\n",
    "    cost = (-1/m)*np.sum(Y*np.log(A)+(1-Y)*np.log(1-A), axis = 1)\n",
    "    #Se calcula el costo sin usar producto punto, por que la suma en automático\n",
    "    #toma cada valor de los vectores y los opera entre si, axis = 1 por que todos son\n",
    "    #vectores de una dimensión\n",
    "\n",
    "    #--Propagación hacia atrás:\n",
    "    dw = (1/m)*np.dot(X, (A-Y).T)\n",
    "    db = (1/m)*np.sum(A-Y, axis=1)[0]\n",
    "    ##Procedemos a liberar el numpy para que salga como un escalar:\n",
    "    cost = np.squeeze(np.array(cost))\n",
    "\n",
    "    ##Guardamos los gradientes en un diccionario\n",
    "    grads = {\n",
    "        'dw':dw,\n",
    "        'db':db\n",
    "    }\n",
    "    return grads, cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Esta función, recibirá como parámetros el w, b, X e Y y hará la propagación hacia delante y hacia atrás."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Función de optimización"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Esta función, optimizará el modelo y hará las actualizaciones de cada delta en las iteraciones. Recordemos las ecuaciones de este caso:\n",
    "\n",
    "- $w:=w-\\alpha \\frac{dJ(w, b)}{dw}$\n",
    "\n",
    "<br>\n",
    "\n",
    "- $b:=b-\\alpha \\frac{dJ(w, b)}{db}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "def optimize(w, b, X, Y, num_iter = 100, lr = 0.009, print_cost = False):\n",
    "    ## Hacemos copia de los parámetros iniciales:\n",
    "    w = copy.deepcopy(w)\n",
    "    b = copy.deepcopy(b)\n",
    "    costs = []\n",
    "    for i in range(num_iter):\n",
    "        grads, cost = propagate(w, b, X, Y)\n",
    "\n",
    "        dw = grads['dw']\n",
    "        db = grads['db']\n",
    "        \n",
    "        w += -lr*dw\n",
    "        b += -lr*db\n",
    "\n",
    "        if i%100 == 0:\n",
    "            costs.append(cost)\n",
    "\n",
    "            if print_cost:\n",
    "                print('Costo después de iteración %i: %f' %(i, cost))\n",
    "\n",
    "    params = {\n",
    "        'w':w,\n",
    "        'b':b\n",
    "    }\n",
    "\n",
    "    grads = {\n",
    "        'dw':dw,\n",
    "        'db':db\n",
    "    }\n",
    "\n",
    "    return params, grads, costs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Con esta función, se garantiza que, en cada iteración, minimize la función y converga a un óptimo global de manera que encontremos los parámetros idóneos.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Predict Function:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se debe crear, finalmente, la función que haga la predicción, luego de calculado los parámetros:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(w, b, X):\n",
    "    m = X.shape[1]\n",
    "    Y_prediction = np.zeros((1, m))\n",
    "    w = w.reshape(X.shape[0], 1)\n",
    "\n",
    "    A = sigmoid(np.dot(w.T, X)+b)\n",
    "\n",
    "    for i in range(A.shape[1]):\n",
    "        if A[0, i]>0.5:\n",
    "            Y_prediction[0, i] = 1\n",
    "        else:\n",
    "            Y_prediction[0, i] = 0\n",
    "    \n",
    "    return Y_prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En general, esta función cálcula el **$\\hat{y}$** en la función sigmoide y en caso tal la probabilidad es mayor de 0.5, devuelve 1, en caso contrario, es 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Modelo!!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Por último, se debe incluir todas estas funciones dentro del modelo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(X_train, Y_train, X_test, Y_test, num_iter = 20000, lr = 0.5, print_cost = False):\n",
    "    w, b = initialize_with_zeros(X_train.shape[0])\n",
    "    ## ¿Porqué es cero?: Recordemos que el X_train no es más que\n",
    "    ## una matriz de tamaño (nx, m), siendo que w y b son de tamaño nx\n",
    "    params, grads, costs = optimize(w, b, X_train, Y_train, num_iter, lr, print_cost)\n",
    "\n",
    "    w, b = params['w'], params['b']\n",
    "\n",
    "    Y_prediction_test = predict(w, b, X_test)\n",
    "    Y_prediction_train = predict(w, b, X_train)\n",
    "    if print_cost:\n",
    "        print(\"train accuracy: {} %\".format(100 - np.mean(np.abs(Y_prediction_train - Y_train)) * 100))\n",
    "        print(\"test accuracy: {} %\".format(100 - np.mean(np.abs(Y_prediction_test - Y_test)) * 100))\n",
    "\n",
    "    \n",
    "    d = {\"costs\": costs,\n",
    "         \"Y_prediction_test\": Y_prediction_test, \n",
    "         \"Y_prediction_train\" : Y_prediction_train, \n",
    "         \"w\" : w, \n",
    "         \"b\" : b,\n",
    "         \"learning_rate\" : lr,\n",
    "         \"num_iterations\": num_iter}\n",
    "    \n",
    "    return d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finalmente, luego de correr el modelo, obtuvimos los siguientes resultados:\n",
    "\n",
    "<img src = pred_1.png>\n",
    "\n",
    "Esto quiere decir que, obtuvimos en el train, un fit del 99%, pero, en el test, obtuvimos un 70%. Esto es bueno, sin embargo también nos da a entender que podemos tener overfitting en nuestra información."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Week 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Networks Overview:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este apartado, compararemos la versión inicial que hicimos las semanas pasadas, con la actual. Donde, haciamos una simple \"red neuronal\" que calculaba nuestro z, luego a. y luego la función de pérdida. En este caso, una red neuronal de al menos dos capas, se ve de la siguiente manera:\n",
    "\n",
    "<img src = NN_1.png>\n",
    "\n",
    "En este caso, la primera capa de NN, se representa de la siguiente manera en la notación:\n",
    "\n",
    "- $z^{[1]} = W^{[1]}x + b^{[1]} \\to a^{[1]} = \\sigma(z^{[1]})$\n",
    "\n",
    "La segunda capa, por su parte, luego del cálculo de la primera función, hace lo siguiente:\n",
    "\n",
    "- $z^{[2]} = W^{[2]}a^{[1]} + b^{[2]} \\to a^{[2]} = \\sigma(z^{[2]})$\n",
    "\n",
    "Cada capa cálcula un z que luego pasa a la siguiente. A la final, devolverá un $a^{[2]}$ que será al final, el que calculará la función de pérdida.\n",
    "\n",
    "Similar que en regresión logística, hará una función de propagación hacia atrás. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Networks Representation:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tomemos inicialmente esta imagen como representación de lo que se está haciendo:\n",
    "\n",
    "<img src = NN_2.png>\n",
    "\n",
    "- Las $x_1, x_2, x_3$ son denominadas las capas de entrada.\n",
    "\n",
    "- La otra capa denotada por los 4 círculos, se denomina **capa oculta/hidden layer** y finalmente, el círculo del final es la capa de salida o **output layer**, es la responsable de cálcular el valor final de $\\hat(y)$.\n",
    "\n",
    "Un dataset, recordemos que está compuesto por los input layer, los datos de valor X y el output layer, o los y que queremos predecir. \n",
    "\n",
    "Entonces **¿qué significa hidden layer?**: Se refiere al hecho de que los valores del training set no son vistos por el usuario dentro de los nodos. Ve cuales deberían ser los input y los outputs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Notación"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para el caso de la notación, en este caso usamos:\n",
    "\n",
    "$a^{[0]} = X$\n",
    "\n",
    "Esto es principalmente por que pasa a llamarse activación del input layer.\n",
    "\n",
    "La capa oculta, empezará a generar activaciones que se denotan por el símbolo $a^{[1]}$\n",
    "\n",
    "Finalmente, tenemos la red neuronal generalizada de la siguiente manera:\n",
    "\n",
    "<img src = NN_3.png>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computing Neural Networks Output:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para representar el cálculo en de una red neuronal de 2 capas, es preciso recordar como está representado nuestra red neuronal clásica construida en la semana pasada:\n",
    "\n",
    "<img src = 'NN_4.png'>\n",
    "\n",
    "Teniendo en cuenta que:\n",
    "- a significa la función de activación y Z el forward pass de la neurona.\n",
    "\n",
    "Ahora bien, para una red neuronal como la revisada en el tópico pasado, de **2 Layers NN**, tengamos en cuenta lo siguiente, la ecuación de la primera neurona de la primera capa se representaría de la siguiente manera:\n",
    "\n",
    "- $z^{[1]}_1 = w^{[1]T}_{1}*x + b^{[1]}_1 \\to a^{[1]}_1 = \\sigma(z^{[1]}_1)$\n",
    "\n",
    "Dónde:\n",
    "\n",
    "- $a^{[l]}_i$ l: capa donde está la neurona e i es el nodo en esa capa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para el resto de nodos, tendríamos n ecuaciones para n nodos, lo que nos dejaría así:\n",
    "\n",
    "<img src = 'NN_5.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora bien, todo esto en un for loop, podríamos ser bastante ineficientes. Para ello, es necesario que se vectoricen todas las variables.\n",
    "\n",
    "Entonces, cogemos cada w traspuesto y podemos unirlos todos en una matriz, que tendría 4 filas (para el caso de esta red con sólo 4 nodos) y 3 columnas, esto se puede hacer producto punto con el vector de x y se suma la constante, dejandonos una matriz de 4x3 para este ejemplo, con todas nuestras ecuaciones y cálculos de Z. \n",
    "\n",
    "Detalles a tener en cuenta:\n",
    "\n",
    "- X para el caso de las imágenes, es un vector con $m - training/test$ elementos (ya transformado, que originalmente es de tamaño (m, nx, 3) donde 3 son las capas rgb de la matriz), y, siendo que, $w^T$, por su parte es un vector fila de tamaño (nx, 1).\n",
    "\n",
    "- Teniendo esta notación principal, si siempre definimos cual sería la cantidad de nodos en nuestro layer con 4, en realidad el shape de esta matriz, sería de tamaño (4, m).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Al final, vamos tener un vector columna llamado $z^{[1]}$\n",
    "\n",
    "Este vector, luego es utilizado para calcular $a^{[1]}$ el cual al final, servirá para la siguiente función de activación de la capa:\n",
    "\n",
    "- $z^{[2]} = W^{[2]}a^{[1]} + b^{[2]} \\to a^{[2]} = \\sigma(z^{[2]})$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectorizing Across Multiple Examples:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Resumiendo lo visto en el tópico anterior, tendríamos:\n",
    "\n",
    "<img src = 'NN_6.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora bien, esto no es óptimo ponerlo directamente en un for loop, por lo cual, es importante que tengamos en cuenta lo anterior, simplemente vectorizando, podemos lograr que tenga la mejor performance posible en temas de código:\n",
    "\n",
    "<img src = 'NN_7.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Funciones de activación"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En las pasadas lecturas, hemos estado usando la función sigmoide, sin embargo, la diferencia entre esta función y las otras, dependerá bastante en el performance que tengamos en nuestra red neuronal. Por ejemplo, podemos usar la función $g$ la cual es igual a $\\tanh(z)$.\n",
    "\n",
    "Algunas rules of thumb que pueden servir para elegir función de activación:\n",
    "\n",
    "- Si tu salida es una clasificación binaria, lo mejor es utilizar ReLU como función de activación para las capas ocultas o la de salida.\n",
    "\n",
    "- Nunca usar la función sigmoide o la g en la capa de salida."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4,)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "x = np.random.rand(4, 5)\n",
    "\n",
    "y = np.sum(x, axis=1)\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src = NN_8.png>\n",
    "<img src = NN_9.png>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Week 3 Assignment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El assingnment program de esta semana, cubre un set de datos que es un plano con colores azules y rojos:\n",
    "\n",
    "<img src = 'week3_1.png'>\n",
    "\n",
    "Aquí, podemos utilizar regresión logística, esto nos daría un accuracy del 47% con nuestro dataset:\n",
    "\n",
    "<img src = 'week3_2.png'>\n",
    "\n",
    "Ahora bien, lo que haremos, en vez de usar regresión logística, será una red neuronal de una capa como la que se muestra a continuación:\n",
    "\n",
    "<img src = 'week3_3.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Las ecuaciones previamente repasadas, son las siguientes:\n",
    "\n",
    "**Mathematically**:\n",
    "\n",
    "For one example $x^{(i)}$:\n",
    "$$z^{[1] (i)} =  W^{[1]} x^{(i)} + b^{[1]}\\tag{1}$$ \n",
    "$$a^{[1] (i)} = \\tanh(z^{[1] (i)})\\tag{2}$$\n",
    "$$z^{[2] (i)} = W^{[2]} a^{[1] (i)} + b^{[2]}\\tag{3}$$\n",
    "$$\\hat{y}^{(i)} = a^{[2] (i)} = \\sigma(z^{ [2] (i)})\\tag{4}$$\n",
    "\n",
    "Given the predictions on all the examples, you can also compute the cost $J$ as follows: \n",
    "$$J = - \\frac{1}{m} \\sum\\limits_{i = 0}^{m} \\large\\left(\\small y^{(i)}\\log\\left(a^{[2] (i)}\\right) + (1-y^{(i)})\\log\\left(1- a^{[2] (i)}\\right)  \\large  \\right) \\small \\tag{6}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora bien, definimos inicialmente, cual sería el número de layers de nuestra red neuronal:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def layer_sizes(X, Y, n_h):\n",
    "    n_x = X.shape[0] #Recordemos que nuestro dataset es de (nx/ny, m), entonces, la capa de entrada es nx\n",
    "    n_h = n_h\n",
    "    n_y = Y.shape[0]\n",
    "\n",
    "    return (n_x, n_h, n_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adicional, debemos definir como vamos a inicializar nuestros parámetros, para ello, usamos la función \n",
    "`np.random.randn(a,b) * 0.01`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_parameters(n_x, n_h, n_y):\n",
    "    W1 = np.random.randn(n_h, n_x)*0.01 #Pesos iniciales son numero de neuronas, y número de features (variables)\n",
    "    b1 = np.zeros((n_h, 1)) #Bias iniciales, es un vector columna, de tamaño número de neuronas\n",
    "    W2 = np.random.randn(n_y, n_h)*0.01 # Pesos de la capa de salida, es el número de ys y el número de capas de la anterior layer\n",
    "    b2 = np.zeros((n_y, 1)) # Bias, es un vector columna del número de ys y el 1\n",
    "\n",
    "    ## Recordemos que las matrices tanto W1 como W2 ya están transpuestas.\n",
    "\n",
    "    parameters = {\"W1\": W1,\n",
    "                  \"b1\": b1,\n",
    "                  \"W2\": W2,\n",
    "                  \"b2\": b2}\n",
    "\n",
    "    return parameters\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora, calcularemos la propagación hacia delante de nuestra red neuronal. En ese caso, usamos:\n",
    "\n",
    "$$Z^{[1]} =  W^{[1]} X + b^{[1]}\\tag{1}$$ \n",
    "$$A^{[1]} = \\tanh(Z^{[1]})\\tag{2}$$\n",
    "$$Z^{[2]} = W^{[2]} A^{[1]} + b^{[2]}\\tag{3}$$\n",
    "$$\\hat{Y} = A^{[2]} = \\sigma(Z^{[2]})\\tag{4}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_propagation(X, parameters):\n",
    "    W1 = parameters['W1']\n",
    "    b1 = parameters['b1']\n",
    "    W2 = parameters['W2']\n",
    "    b2 = parameters['b2']\n",
    "\n",
    "    Z1 = np.dot(W1, X) + b1\n",
    "    A1 = np.tanh(Z1)\n",
    "    Z2 = np.dot(W1, A1) + b2\n",
    "    A2 = sigmoid(Z2)\n",
    "    cache = {\"Z1\": Z1,\n",
    "             \"A1\": A1,\n",
    "             \"Z2\": Z2,\n",
    "             \"A2\": A2}\n",
    "    \n",
    "    return A2, cache"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Luego de la propagación hacia delante, calculamos el costo:\n",
    "\n",
    "$$J = - \\frac{1}{m} \\sum\\limits_{i = 1}^{m} \\large{(} \\small y^{(i)}\\log\\left(a^{[2] (i)}\\right) + (1-y^{(i)})\\log\\left(1- a^{[2] (i)}\\right) \\large{)} \\small\\tag{13}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def costs(A2, Y):\n",
    "    m = Y.shape[1]\n",
    "    logprobs = np.multiply(np.log(A2), Y) + (1-Y)*np.log(1-A2)\n",
    "    cost = (-1/m) * np.sum(logprobs)\n",
    "\n",
    "    cost = float(np.squeeze(cost))\n",
    "\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora, una vez calculamos la propagación hacia delante y el costo, debemos hacerlo hacia atrás, recordemos que usamos el algoritmo de descenso del gradiente:\n",
    "\n",
    "<img src = NN_9.png>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward_propagation(parameters, cache, X, Y):\n",
    "    m = X.shape[1]\n",
    "    ## Nos traemos todas las variables de cada uno\n",
    "    W1 = parameters['W1']\n",
    "    W2 = parameters['W2']\n",
    "    A1 = cache['A1']\n",
    "    A2 = cache['A2']\n",
    "    dZ2 = A2 - Y\n",
    "    dW2 = (1/m)*np.dot(dZ2, A1.T)\n",
    "    db2 = (1/m) * np.sum(dZ2, axis=1, keepdims=True)\n",
    "    dZ1 = np.dot(W2.T, dZ2)*(1-np.power(A1, 2))\n",
    "    dW1 = (1/m) * np.dot(dZ1, X.T)\n",
    "    db1 = (1/m)*np.sum(dZ1, axis = 1, keepdims=True)\n",
    "    grads = {\"dW1\": dW1,\n",
    "             \"db1\": db1,\n",
    "             \"dW2\": dW2,\n",
    "             \"db2\": db2}\n",
    "    return grads"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usamos las ecuaciones de la derecha, pero estas son completamente adaptables a redes neuronales más complejas con varias capas. Se debe sólo cambiar la función de activación y tener en cuenta la derivada de la función."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recordemos, que una vez tengamos la propagación hacia delante y hacia atrás, debemos actualizar los parámetros:\n",
    "\n",
    "$\\theta = \\theta - \\alpha \\frac{\\partial J }{ \\partial \\theta }$ dónde  $\\alpha$ es el lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_parameters(parameters, grads, lr):\n",
    "    W1 = copy.deepcopy(parameters['W1'])\n",
    "    b1 = copy.deepcopy(parameters['b1'])\n",
    "    W2 = copy.deepcopy(parameters['W2'])\n",
    "    b2 = copy.deepcopy(parameters['b2'])\n",
    "    dW1 = grads['dW1']\n",
    "    db1 = grads['db1']\n",
    "    dW2 = grads['dW2']\n",
    "    db2 = grads['db2']\n",
    "    W1 += -lr*dW1\n",
    "    b1 += -lr*db1\n",
    "    W2 += -lr*dW2\n",
    "    b2 += -lr*db2\n",
    "\n",
    "    parameters = {\"W1\": W1,\n",
    "                  \"b1\": b1,\n",
    "                  \"W2\": W2,\n",
    "                  \"b2\": b2}\n",
    "\n",
    "    ## En cada iteración, esto nos va a devolver los parámetros usando el algoritmo de descenso del gradiente\n",
    "    return parameters\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora, integraremos todo en una función de general llamada nn_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nn_model(X, Y, n_h, num_iterations, print_cost, lr):\n",
    "    n_x = layer_sizes(X, Y, n_h = n_h)[0]\n",
    "    n_y = layer_sizes(X, Y, n_h = n_h)[2]\n",
    "\n",
    "    parameters = initialize_parameters(n_x, n_h, n_y)\n",
    "\n",
    "    ## Aquí, hacemos el algoritmo de descenso del gradiente:\n",
    "\n",
    "    for i in range(0, num_iterations):\n",
    "\n",
    "        ## forward propagation ->\n",
    "        A2, cache = forward_propagation(X, parameters)\n",
    "\n",
    "        cost = costs(A2, Y)\n",
    "\n",
    "        ## <- backward propagation\n",
    "\n",
    "        grads = backward_propagation(parameters, cache, X, Y)\n",
    "\n",
    "        ## Updating weights:\n",
    "\n",
    "        parameters = update_parameters(parameters, grads, lr)\n",
    "        if print_cost and i % 1000 == 0:\n",
    "            print (\"Cost after iteration %i: %f\" %(i, cost))\n",
    "\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Al probarlo con nuestro dataset y testear todo el modelo, obtenemos lo siguiente:\n",
    "\n",
    "<img src = 'week3_4.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se debe generar la función de predicción, para ello, tomamos los valores de A2 y verificamos con propagación hacia delante, cuales tienen probabilidades mayores a 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(parameters, X):\n",
    "    A2, cache = forward_propagation(X, parameters)\n",
    "    predictions = (A2>0.5)\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ya con esto, testeamos el modelo y nos da la siguiente información:\n",
    "\n",
    "<img src = 'week3_5.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como observamos, para 10000 iteraciones, tenemos un gráfico que es capaz de clasificar cada color donde corresponde, con una precisión del 90%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Week 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deep L-layer Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En las semanas pasadas, vimos dos tipos de redes neuronales, una red neuronal de una sola capa.\n",
    "\n",
    "<img src = 'NN_10.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora bien, más de 5 layers, son consideradas por la comunidad como redes neuronales más profundas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para el ejemplo de ahora, usaremos el siguiente ejemplo para la notación:\n",
    "\n",
    "<img src = 'NN_11.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Notation:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este caso, usaremos los siguientes simbolos para representar diferentes elementos:\n",
    "\n",
    "- $L$ = número de capas, siendo 4 para el ejemplo\n",
    "- $n^{[l]}$ = número de unidades en la capa $L$, para el ejemplo: $n^{[1]} = 5$, $n^{[3]} = 3$\n",
    "\n",
    "En propagación hacia delante, también tenemos activacion en cada capa:\n",
    "\n",
    "- $a^{[l]} = g^{[l]}(z^{[l]}) \\to $ no es más que la activación\n",
    "- $W^{[1]}$ = a los pesos para $z^{[l]}$\n",
    "- $b^{[l]}$ = el bias de cada una de las capas l\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forward propagation in a deep network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este caso, las ecuaciones son similares a como las venimos utilizando en una red neuronal de dos capas:\n",
    "\n",
    "- $z^{[1]} = W^{[1]}X + b^{[1]} \\to a^{[1]} = g^{[1]}(z^{[1]})$\n",
    "\n",
    "Y así para cada capa, de forma general, tenemos que:\n",
    "\n",
    "- $z^{[l]} = w^{[l]}a^{[l-1]} + b^{[l]} \\to a^{[l]} = g^{[l]}(z^{[l]})$\n",
    "\n",
    "Recordemos que $a^{[0]} = X$\n",
    "\n",
    "La vectorización de esto sería:\n",
    "- $Z^{[l]} = w^{[l]}A^{[l-1]} + b^{[l]} \\to A^{[l]} = g^{[l]}(Z^{[l]})$\n",
    "\n",
    "En este caso, estaría bien implementar un forloop para el número L de hidden layers. Teniendo en cuenta que para la última, si es necesario generar un cálculo diferente."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting your matriz Dimensions Right"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos obtener con lo aprendido, una aproximación de esta matriz:\n",
    "\n",
    "<img src = 'NN_11.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora bien, nuestro $A^{[0]}$ bien será una matriz de (3, m) que recibirá la primera capa. En este caso:\n",
    "- la matriz $W^{[1]}$ será de tamaño (5, 3), \n",
    "- X ya vimos que es de tamaño (3, m) y $b^{[1]}$ sería un vector columna de (5,1). \n",
    "- Haciendo los cálculos para obtener Z, tendriamos que $Z^{[1]}$ sería igual a (5, m) en conjunto con $A^{[1]}$. \n",
    "- Ahora, haciendo la siguiente propagación hacia delante, nuevamente $W^{[2]}$ sería igual a (5, 5) ya que recibe 5 diferentes parámetros en cada hidden layer provenientes de la capa anterior.\n",
    "- ya tenemos el tamaño de $A^{[1]}$, y $b^{[2]}$ seguiría siendo igual. Haciendo los cálculos, nuevamente $Z^{[2]}$ y $A^{[2]}$ serían de tamaño (5, m).\n",
    "\n",
    "- En la última capa de 3 unidades, tendriamos que $W^{[3]}$ debe ser de parámetros (3, 5) ya que nuevamente recibe 5 parámetros. \n",
    "\n",
    "- $b^{[3]}$ cambia esta vez a ser de tamaño (3, 1)\n",
    "\n",
    "- Esta vez $Z^{[3]}$ y $A^{[3]}$ serán de tamaño (3, m)\n",
    "\n",
    "- Moviendonos hacia delantre hasta el output layer, tenemos un $W^{[4]}$ de tamaño (1, 3), siendo esta vez que $b^{[4]}$ es de tamaño (1,1) y la salida $Z^{[4]}$ y $A^{[4]}$ son de tamaño (1, m)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### De esto, podemos obtener una fórmula general:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Forward propagation**\n",
    "\n",
    "- $W^{[l]} : (n^{[l]}, n^{[l-1]})$\n",
    "\n",
    "- $b^{[l]} : (n^{[l]}, 1)$\n",
    "\n",
    "- $Z^{[l]}, A^{[l]} : (n^{[l]}, m)$\n",
    "\n",
    "**Backward propagation**\n",
    "\n",
    "- $dW^{[l]} : (n^{[l]}, n^{[l-1]})$\n",
    "\n",
    "- $db^{[l]} : (n^{[l]}, 1)$\n",
    "\n",
    "- $dZ^{[l]}, dA^{[l]} : (n^{[l]}, m)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why deep representations?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Por que las representaciones profundas de redes neuronales son importantes:\n",
    "\n",
    "Cada capa estará aprendiendo una función diferente. Una puede estar aprendiendo todo acerca de los bordes, la siguiente sólo ojos, boca, nariz y la penúltima, tendría que reconocer la cara completa.\n",
    "\n",
    "Aparece la teoría de circuitos y aprendizaje profundo. \n",
    "\n",
    "Hay funciones que puedes cálcular con una red neuronal profunda pero pequeña, que probablemente una red neuronal shallow requerirá exponencialmente más unidades ocultas para calcular.Es decir, una red neuronal profunda, donde cada capa se encarga de ir haciendo diferentes cálculos, pero con pocas unidades ocultas, rendirá mucho mejor que una shallow con muchísimas mas unidades ocultas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usando la bigO notation, cada XOR será de largo O(logn), pero una O(2^n) será sólo con una sóla capa."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building Blocks of Deep Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Las ecuaciones generales que usaremos en este caso, serían:\n",
    "\n",
    "- $dZ^{[l]} = dA^{[l]}*g^{[l]'}(Z[l])$\n",
    "\n",
    "- $dW^{[l]} = \\frac{1}{m} dZ^{[l]}.A^{[l-1]T}$\n",
    "\n",
    "- $db^{[l]} = \\frac{1}{m} np.sum(dZ^{[l]}, axis = 1, keepdims = True)$\n",
    "\n",
    "- $dA^{[l-1]} = W^{[l]T}.dZ^{[l]}$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hiper parametros y parametros"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Los parámetros en nuestra red neuronal son:\n",
    "\n",
    "- $W^{[1]}, b^{[1]}... W^{[L]}, b^{[L]}$\n",
    "\n",
    "Los **hiperparámetros** son:\n",
    "\n",
    "- $\\alpha, iter, L, n^{[1]}, n^{[2]} ... n^{[L]}$\n",
    "\n",
    "Siendo alpha el learning rate, iter, el número de iteraciones, y n el número de unidades ocultas\n",
    "\n",
    "Otros hiperparámetros pueden ser:\n",
    "\n",
    "- Momentum, minimum batch size, regularization."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4 (tags/v3.10.4:9d38120, Mar 23 2022, 23:13:41) [MSC v.1929 64 bit (AMD64)]"
  },
  "vscode": {
   "interpreter": {
    "hash": "3588fb9ea7dff2eb2287700cb5e34b8525e27f87c085349e2456f200952a0a0a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
